{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Jupyer Notebook for this analysis since it is easier and faster than standard python scripts, in particular when we need to draw plots.\n",
    "\n",
    "Other than plots and insights about some distribution, a small fraction of feature extraction is performed at the end.\n",
    "- the number of tokens in text fields \n",
    "- info from urls\n",
    "\n",
    "This task is done also via python scripts, much better way to keep track of the experiments and to speed up the eventual productionalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns #visualisation\n",
    "import matplotlib.pyplot as plt #visualisation\n",
    "%matplotlib inline \n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find path input data file (check readme.txt in folder \"/data\" for more info about how to get this file)\n",
    "notebooks_folder_path = os.path.abspath('') # /notebooks\n",
    "data_folder_path = os.path.join(notebooks_folder_path, '..', 'data') # /notebooks\n",
    "\n",
    "path_data = os.path.join(data_folder_path, 'data_redacted.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it into df\n",
    "df = pd.read_csv(path_data, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_rows_df = df[df.duplicated()]\n",
    "duplicate_rows_df.shape\n",
    "\n",
    "# there are no duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the null values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task is to classify the category \"starting\" from the columns \"title\", \"text\" and \"url\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets start to see the \"category\" distribution\n",
    "df['category'].value_counts().plot(kind='bar', figsize=(10,5))\n",
    "plt.title(\"Category Distribution\")\n",
    "\n",
    "# The distribution is not uniform: it is quite unbalanced even if not too drammatically. \n",
    "# Anyway I need to take care about this in the metrics..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title and Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"title\" and \"text\" are quite straigthforward fields for news classification\n",
    "\n",
    "# However, the len of the text can be pretty informative from my experience\n",
    "# lets see the distribution of the number of words in those fields\n",
    "\n",
    "df['title_len'] = df['title'].str.len()\n",
    "df['text_len'] = df['text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some basic statistics about title_len\n",
    "df['title_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some basic statistics about title_len\n",
    "df['text_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df['title_len'])\n",
    "\n",
    "# title is usually a short string\n",
    "# max number of tokens < 512, so I dont see any limitations for the kind of model I want use use next for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df['text_len'])\n",
    "\n",
    "# text is much longer than the title...\n",
    "# Considering all the text can be a problem since a lot of computing power is needed resulting in\n",
    "#       higher costs\n",
    "#       slower inferences\n",
    "#       and maybe a not so significant improvemnt of the accuracy\n",
    "\n",
    "# From my experience, it is better to truncate the text (i take the first N tokens), \n",
    "# but i can consider to keep the last N or whatever...\n",
    "\n",
    "# here there is a thread about text classification for long texts using transformer\n",
    "# https://stackoverflow.com/questions/58636587/how-to-use-bert-for-long-text-classification?rq=1\n",
    "\n",
    "# Along the use of Reformer or Longformer, it is recommended a truncation anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does categories have different len words distributions? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"category\", y=\"title_len\", data=df)\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"category\", y=\"text_len\", data=df)\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(\n",
    "    data=df,\n",
    "    x=\"text_len\", y=\"title_len\", hue=\"category\",\n",
    "    kind=\"kde\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len words distributions can be very different across the category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(\n",
    "    data=df[df['category'].isin(['sports', 'fashion_beauty_lifestyle', 'technology_science', 'cars_motors'])],\n",
    "    x=\"text_len\", y=\"title_len\", hue=\"category\",\n",
    "    kind=\"kde\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(50)['text']\n",
    "\n",
    "# english seems to be the only language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"url\" is less readable and it can hide precious information... \n",
    "# I want to see if I can get some useful insigths I can exploit in the ML model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some urls\n",
    "df['url'].tolist()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url can contain very useful information!\n",
    "#     Some domains are repeated and frequent\n",
    "#     From some sources, I expect a certain bias \n",
    "#           example from www.sciencedaily.com i can expect many news about \"technology_science\"\n",
    "#     In the path of the url there are some keywords I can use\n",
    "#           take a look at the example below where i see \"beauty\" in the url...\n",
    "\n",
    "example_url = df[['url', 'category']].iloc[4]['url']\n",
    "example_category = df[['url', 'category']].iloc[4]['category']\n",
    "\n",
    "print(example_url)\n",
    "print(example_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i will try to parse those urls\n",
    "# https://docs.python.org/3/library/urllib.parse.html\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some examples taken randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_url = df['url'].iloc[100]\n",
    "urlparse(example_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_url = df['url'].iloc[105]\n",
    "urlparse(example_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_url = df['url'].iloc[21]\n",
    "urlparse(example_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"netloc\" and \"path\" are probably the most useful info I could need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['urlparse'] = df['url'].apply(urlparse)\n",
    "df['netloc'] = df['urlparse'].apply(lambda x: x.netloc)\n",
    "df['netloc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['netloc'].value_counts()\n",
    "\n",
    "# there are many repetions for some netlocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netloc_count = df['netloc'].value_counts()\n",
    "different_netlocs = len(netloc_count)\n",
    "different_netlocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count distribution\n",
    "netloc_count.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are a few domains very frequent, while many of them appears just 1,2 or 3 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# focus\n",
    "netloc_count[:40].plot()\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netloc_count[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will keep the \"netloc\" for the most frequent labels\n",
    "# I'll map all the rest in a fictious label called \"Other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['netloc_mod'] = df['netloc'].mask(df['netloc'].map(df['netloc'].value_counts()) < 50, 'Other')\n",
    "df['netloc_mod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the new distribution of the netloc\n",
    "new_count_distr = df['netloc_mod'].value_counts()\n",
    "new_count_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(20.7,8.27)})\n",
    "ax = sns.countplot(x=\"netloc_mod\", hue=\"category\", data=df)\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from this plot i can see that there are dependencies between the category and the netloc as the intuition suggests\n",
    "#      from phys.org i see only science related news (as well as for wired.co.uk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after the netloc, i can extract the path \n",
    "df['path'] = df['urlparse'].apply(lambda x: x.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['path'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from urls i can also extract query and params, but they dont seem useful as you can see below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['query'] = df['urlparse'].apply(lambda x: x.query)\n",
    "df['query'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['params'] = df['urlparse'].apply(lambda x: x.params)\n",
    "df['params'].value_counts()[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
